model_cfg: configs/models/fm-b-220m.json
data_catalog: data/catalog.yaml
mixture: configs/data/mixtures/base_mix.yaml
precision: bf16
optimizer:
  name: adamw
  betas: [0.9, 0.95]
  weight_decay: 0.1
  lr: 2.0e-4
  warmup_steps: 4000
  schedule: cosine
distributed:
  backend: nccl
  ddp: true
  fsdp: false
  fsdp_policy: full_shard
  grad_accum_steps: 16
  gradient_clip: 1.0
logging:
  wandb: false
  tensorboard: true
  modal_ledger_interval: 10
  log_every: 50
losses:
  lambda_geo: 0.0
  modal:
    apply_on: ["planner"]
    lambda_mod: 0.25
    lambda_planner: 1.0
    lambda_token: 0.0
    tau_planner: 1.0
    tau_token: 1.0
    slot_len: 512
    view_window: 1
    slot_weights: [1.0, 0.5]
    stop_grad_projection: true
    eps: 1.0e-6
    clip_kl: 10.0
    token_topk: 256
ledger:
  lambda_mod: 0.25
  apply_on: ["planner", "token_spans"]
  span_tokens: ["<PLAN_START>", "<PLAN_END>"]
  view:
    slot_len: 512
    window: 1
gate:
  armijo_alpha: 1.0e-4
  trust_radius_init: 0.01
  kl_smooth_max: 1.0
  stability_slice_ratio: 0.125
  backtrack_factor: 0.5
  widen_view_on_reject: true
  max_view_window: 3
checkpoints:
  output_dir: ckpts/exp-pretrain-v1
  save_every_steps: 2000
  save_every_tokens: 50000000
  keep_last: 5
  resume: auto
engine:
  enabled: false
  eval_interval: 2000
  batch_size: 8
  qfc_tolerance: 1.0e-8
